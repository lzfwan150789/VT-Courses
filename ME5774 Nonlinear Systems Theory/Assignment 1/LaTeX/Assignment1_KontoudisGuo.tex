
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{pifont}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{array}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Homework 1}
\author{Jia Guo, Georgios Kontoudis\\ 
ME5774 Nonlinear Systems Theory\\
Professor Cornel Sultan} 
\date{Fall 2017}
 
\maketitle
\begin{problem}{1} %theorem, exercise, problem, or question 
Show that the set of real matrices
\begin{equation*}
SO(2)=\{ R\in \mathbb{R}^{2 \times 2} |R^{-1}=R^\top, det(R)=1 \},
\end{equation*}
is a group under matrix multiplication. Is this an Abelian group?
\end{problem}
\begin{solution}
A group is called Abelian if it satisfies all the group properties and the commutative property. 

First, we study the \textbf{closure} axiom for multiplication $R_1\times R_2 \in \mathbb{R}^{2 \times 2}$ for all $R_1,R_2 \in \mathbb{R}^{2 \times 2}$. Indeed, the property holds as the multiplication of two matrices $2 \times 2$ results a $2 \times 2$ matrix. Note that this property is valid only for square matrices. More specifically let $R_1,R_2 \in SO(2)$ and the multiplication yields
\begin{equation*}
\begin{aligned}
& R_1 R_2
&  =
&&& \begin{bmatrix}
cos\theta_1 & sin\theta_1 \\
-sin\theta_1 & cos\theta_1
\end{bmatrix}\begin{bmatrix}
cos\theta_2 & sin\theta_2 \\
-sin\theta_2 & cos\theta_2
\end{bmatrix}\\
&& =
&&& \begin{bmatrix}
cos\theta_1cos\theta_2-sin\theta_1sin\theta_2 & cos\theta_1sin\theta_2+sin\theta_1cos\theta_2 \\
-sin\theta_1cos\theta_2-cos\theta_1sin\theta_2 & -sin\theta_1sin\theta_2+cos\theta_1cos\theta_2
\end{bmatrix}\\ 
&& =
&&&  \begin{bmatrix}
cos(\theta_1+\theta_2) & sin(\theta_1+\theta_2) \\
-sin(\theta_1+\theta_2) & cos(\theta_1+\theta_2)
\end{bmatrix}=R_{12}.\\
\end{aligned}
\end{equation*}
Since in the set of real numbers $\mathbb{R}$ the closure property holds for addition, then $\theta_1+ \theta_2 \in \mathbb{R}$ for any $\theta_1, \theta_2 \in \mathbb{R}$. Therefore, $R_{12} \in SO(2)$ for any $R_1,R_2 \in SO(2)$. 

Next, we study the \textbf{associativity} of multiplication in $SO(2)$. Let $R_f, R_g, R_h \in SO(2)$ where $R_f=f_{il}$, $R_g=g_{lk}$, and $R_h=h_{kj}$, then we get
\begin{equation*}
\begin{aligned}
& ((R_f R_g) R_h)_{ij}
&  =
&&& \sum_{p=1}^k (R_f R_g)_{ip}h_{pj}\\
&& =
&&& \sum_{p=1}^k (\sum_{q=1}^l f_{iq}g_{qp})h_{pj}\\ 
&& =
&&&  \sum_{p=1}^k \sum_{q=1}^l f_{iq}g_{qp}h_{pj}.
\end{aligned}
\end{equation*}
Similarly,
\begin{equation*}
\begin{aligned}
& (R_f (R_g R_h))_{ij}
&  =
&&& \sum_{q=1}^l f_{iq}(R_g R_h))_{qj}\\
&& =
&&& \sum_{q=1}^q f_{iq}(\sum_{p=1}^k g_{lp}h_{pj})\\ 
&& =
&&&  \sum_{q=1}^q \sum_{p=1}^k f_{iq}g_{lp}h_{pj}\\
&& =
&&&  \sum_{p=1}^k \sum_{q=1}^l f_{iq}g_{qp}h_{pj}.
\end{aligned}
\end{equation*}
This means that any matrix multiplication can be associative and thus holds in $SO(2)$.

Regarding  the \textbf{identity} element, we need to show that there exists an element $I \in SO(2)$ such that $RI=IR=R$. Let $R=r_{ij}$, $I=i_{ij}$, and $b_{ij}=RI_{ij}$. We shall prove that $b_{ij}=r_{ij}$ and $r_{ij}=b_{ij}$. The definition of identity matrix determines $i_{ij}=0$ for all $i \neq j$, $i_{ii}=1$, and $det(I)=1$. The matrix multiplication yields
\begin{equation*}
b_{ij}= \sum_{n=1}^m r_{in} i_{nj}=r_{ij}.
\end{equation*}
Similarly,
\begin{equation*}
r_{ij}= \sum_{n=1}^m b_{in} i_{nj}=b_{ij}.
\end{equation*}
Thus, the identity element property holds in $SO(2)$. 

Since the $det(R)\neq 0$, $det(R)=1$  and $R\in \mathbb{R}^{2 \times 2}$ then $R=\begin{bmatrix} 
a & b \\
c & d
\end{bmatrix}$ is \textbf{invertible}
\begin{equation*}
R^{-1}=\frac{1}{det(R)}\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}= \begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix} .
\end{equation*}
We want to prove that $R^{-1}R=RR^{-1}=I$, so
\begin{equation*}
RR^{-1}=\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}=
\begin{bmatrix}
ad-bc & 0 \\
0 & ad-bc
\end{bmatrix}, 
\end{equation*}
since $det(R)=1 \Rightarrow ad-bc=1$, therefore
\begin{equation*}
RR^{-1}=
\begin{bmatrix}
ad-bc & 0 \\
0 & ad-bc
\end{bmatrix} =\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix} =I_{2\times 2}.
\end{equation*}
Similarly, 
\begin{equation*}
R^{-1}R=\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}=
\begin{bmatrix}
ad-bc & 0 \\
0 & ad-bc
\end{bmatrix}\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix} =I_{2\times 2}. 
\end{equation*}
But the latter does not corresponds to $SO(2)$ as $R^{-1} \neq R^{T}$. Let $R \in SO(2)$ then $R^{-1}= R^{T}$. We shall prove that $RR^{T}=R^{T}R=I$
\begin{equation*}
\begin{aligned}
& R R^\top
&  =
&&& \begin{bmatrix}
cos\theta & sin\theta \\
-sin\theta & cos\theta
\end{bmatrix}\begin{bmatrix}
cos\theta & -sin\theta \\
sin\theta & cos\theta
\end{bmatrix}\\
&&  =
&&& \begin{bmatrix}
cos^2\theta+sin^2\theta & -sin\theta cos\theta +cos\theta sin\theta \\
-sin\theta cos\theta+cos\theta sin\theta & sin^2\theta+cos^2\theta
\end{bmatrix}\\
&& =
&&& \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}=I.\\
\end{aligned}
\end{equation*}
Similarly,
\begin{equation*}
\begin{aligned}
& R^\top R 
&  =
&&& \begin{bmatrix}
cos\theta & -sin\theta \\
sin\theta & cos\theta
\end{bmatrix}\begin{bmatrix}
cos\theta & sin\theta \\
-sin\theta & cos\theta
\end{bmatrix}\\
&&  =
&&& \begin{bmatrix}
cos^2\theta+sin^2\theta & cos\theta sin\theta-sin\theta cos\theta \\
cos\theta sin\theta-sin\theta cos\theta & sin^2\theta+cos^2\theta
\end{bmatrix}\\
&& =
&&& \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}=I.\\
\end{aligned}
\end{equation*}
Thus, the inverse element property holds for all $R\in SO(2)$. Since, the CAII properties hold the $SO(2)$ is a group multiplication.

To prove that $SO(2)$ is also \textbf{commutative} let $R_1,R_2 \in SO(2)$ and from the closure property we know that 
\begin{equation*}
\begin{aligned}
& R_1 R_2
&  =
&&&  \begin{bmatrix}
cos(\theta_1+\theta_2) & sin(\theta_1+\theta_2) \\
-sin(\theta_1+\theta_2) & cos(\theta_1+\theta_2)
\end{bmatrix}=R_{12}.
\end{aligned}
\end{equation*}
Similarly,
\begin{equation*}
\begin{aligned}
& R_2 R_1
&  =
&&& \begin{bmatrix}
cos\theta_2 & sin\theta_2 \\
-sin\theta_2 & cos\theta_2
\end{bmatrix}\begin{bmatrix}
cos\theta_1 & sin\theta_1 \\
-sin\theta_1 & cos\theta_1
\end{bmatrix}\\
&& =
&&&  \begin{bmatrix}
cos(\theta_1+\theta_2) & sin(\theta_1+\theta_2) \\
-sin(\theta_1+\theta_2) & cos(\theta_1+\theta_2)
\end{bmatrix}=R_{12}.\\
\end{aligned}
\end{equation*}
Thus, $SO(2)$ is commutative as $R_1R_2=R_2R_1$ and an abelian group.
\end{solution}



\begin{problem}{2} %theorem, exercise, problem, or question
Jia Guo 
\end{problem}



\begin{problem}{3} %theorem, exercise, problem, or question 
Jia Guo
\end{problem}


\begin{problem}{4} %theorem, exercise, problem, or question 
Use Lyapunov's direct method to show that the origin is an assymptotically stable equilibrium for 
\begin{align*}
\dot{x_1}=-tan(x_1)+x_2\\
\dot{x_2}=-tan(x_2)-x_1.
\end{align*}
\end{problem}
\begin{solution}
For the given system we choose a Lyapunov function candidate
\begin{equation*}
V(x_1,x_2)=\frac{1}{2}x_1^2+\frac{1}{2}x_2^2>0,
\end{equation*}
which is positive definite $V>0$ for all $x_1,x_2 \in \mathbb{R}-\{0 \}$ and $V(0,0)=0$. The rate of change in $V$ yields
\begin{equation*}
\begin{aligned}
& \dot{V}
&  =
&&& x_1\dot{x_1}+x_2\dot{x_2}\\
&& =
&&& x_1(-tan(x_1) +x_2)+x_2(-tan(x_2)-x_1)\\
&& =
&&& -(tan(x_1)x_1+tan(x_2)x_2).
\end{aligned}
\end{equation*}
The $x_1,x_2$ have to be confined in the interval of one period, that includes the origin, so that the function is locally Lipschitz. Then, $tan(x)x>0$ for all $x_1,x_2 \in D-\{0 \}$ and $\dot{V}<0$ results a valid Lyapunov function. Thus, the origin is an asymptotically stable equilibrium.



\end{solution}

\end{document}


